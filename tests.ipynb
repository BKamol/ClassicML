{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=14, n_informative=10, noise=15, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f'col_{col}' for col in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=14, n_informative=10, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f'col_{col}' for col in X.columns]\n",
    "df = pd.read_csv('./data/data_banknote_authentication.txt', header=None)\n",
    "df.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'target']\n",
    "X, y = df.iloc[:,:4], df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_banknote_authentication.txt', header=None)\n",
    "df.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'target']\n",
    "X, y = df.iloc[:,:4], df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes(as_frame=True)\n",
    "X, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=150, n_features=14, n_informative=10, noise=15, random_state=42)\n",
    "X = pd.DataFrame(X).round(2)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f'col_{col}' for col in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(n, k):\n",
    "    if 0 <= k <= n:\n",
    "        nn = 1\n",
    "        kk = 1\n",
    "        for t in range(1, min(k, n - k) + 1):\n",
    "            nn *= n\n",
    "            kk *= t\n",
    "            n -= 1\n",
    "        return nn // kk\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "def x(n, m, p):\n",
    "    return (m - n*p) / np.sqrt(n*p*(1-p))\n",
    "\n",
    "def F(x):\n",
    "    return quad(lambda t: np.exp(-t**2/2), 0, x) / np.sqrt(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.value_split = None\n",
    "        self.value_leaf = None\n",
    "        self.side = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.idx = None\n",
    "\n",
    "class MyTreeReg:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20, bins=None, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_leafs = max_leafs\n",
    "        self.leafs_cnt = 1\n",
    "        self.bins = bins\n",
    "        self.__sum_tree_values = 0\n",
    "        self.split_values = {}\n",
    "        self.criterion = criterion\n",
    "        self.fi = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = None\n",
    "        self.fi = { col: 0 for col in X.columns }\n",
    "        \n",
    "        def create_tree(root, X_root, y_root, side='root', depth=0):\n",
    "            if root is None:\n",
    "                root = Node()\n",
    "            col_name, split_value, ig = self.get_best_split(X_root, y_root)\n",
    "\n",
    "            mean_value = y_root.mean()\n",
    "\n",
    "            if depth >= self.max_depth or \\\n",
    "              len(y_root) < self.min_samples_split or \\\n",
    "              (self.leafs_cnt > 1 and self.leafs_cnt >= self.max_leafs):\n",
    "                root.side = side\n",
    "                root.value_leaf = mean_value\n",
    "                root.idx = list(y_root.index)\n",
    "                self.__sum_tree_values += root.value_leaf\n",
    "                return root\n",
    "\n",
    "            self.fi[col_name] += len(y_root) / len(y) * ig\n",
    "\n",
    "            X_left = X_root.loc[X_root[col_name] <= split_value]\n",
    "            y_left = y_root.loc[X_root[col_name] <= split_value]\n",
    "\n",
    "            X_right = X_root.loc[X_root[col_name] > split_value]\n",
    "            y_right = y_root.loc[X_root[col_name] > split_value]\n",
    "\n",
    "            if len(X_left) == 0 or len(X_right) == 0:\n",
    "                root.side = side\n",
    "                root.value_leaf = mean_value\n",
    "                root.idx = list(y_root.index)\n",
    "                self.__sum_tree_values += root.value_leaf\n",
    "                return root\n",
    "\n",
    "            root.feature = col_name\n",
    "            root.value_split = split_value\n",
    "            self.leafs_cnt += 1\n",
    "\n",
    "            root.left = create_tree(root.left, X_left, y_left, 'left', depth + 1)\n",
    "            root.right = create_tree(root.right, X_right, y_right, 'right', depth + 1)\n",
    "\n",
    "            return root\n",
    "\n",
    "        self.tree = create_tree(self.tree, X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for _, row in X.iterrows():\n",
    "            node = self.tree\n",
    "            while node.feature is not None:\n",
    "                if row[node.feature] <= node.value_split:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            y_pred.append(node.value_leaf)\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def print_tree(self, node=None, depth=0):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        if node.feature is not None:\n",
    "            print(f\"{' ' * depth}{node.feature} > {node.value_split}\")\n",
    "            if node.left is not None:\n",
    "                self.print_tree(node.left, depth + 1)\n",
    "            if node.right is not None:\n",
    "                self.print_tree(node.right, depth + 1)\n",
    "        else:\n",
    "            print(f\"{' ' * depth}{node.side} = {node.value_leaf}\")\n",
    "\n",
    "    def get_best_split(self, X, y):\n",
    "        mse_0 = self.mse(y)\n",
    "\n",
    "        col_name = None\n",
    "        split_value = None\n",
    "        gain = -float('inf')\n",
    "\n",
    "        for col in X.columns:\n",
    "            if not (col in self.split_values.keys()):\n",
    "                x_unique_values = np.unique(X[col])\n",
    "                if self.bins is None or len(x_unique_values) - 1 < self.bins:\n",
    "                    self.split_values[col] = np.array([(x_unique_values[i - 1] + \\\n",
    "                    x_unique_values[i]) / 2 for i in range(1, len(x_unique_values))])\n",
    "                else:\n",
    "                    _, self.split_values[col] = np.histogram(X[col], bins=self.bins)\n",
    "\n",
    "            for split_value_i in self.split_values[col]:\n",
    "                mask = X[col] <= split_value_i\n",
    "                left_split, right_split = y[mask], y[~mask]\n",
    "\n",
    "                mse_left = self.mse(left_split)\n",
    "                mse_right = self.mse(right_split)\n",
    "\n",
    "                weight_left = len(left_split) / len(y)\n",
    "                weight_right = len(right_split) / len(y)\n",
    "\n",
    "                mse_i = weight_left * mse_left + weight_right * mse_right\n",
    "\n",
    "                gain_i = mse_0 - mse_i\n",
    "                if gain < gain_i:\n",
    "                    col_name = col\n",
    "                    split_value = split_value_i\n",
    "                    gain = gain_i\n",
    "\n",
    "        return col_name, split_value, gain\n",
    "            \n",
    "    def mse(self, t):\n",
    "        t_mean = np.mean(t)\n",
    "        return np.sum((t - t_mean) ** 2) / (len(t)+1e-15)\n",
    "    \n",
    "    def __node_rule(self, p, split=pd.Series()):\n",
    "        if self.criterion == 'entropy':\n",
    "            return -np.sum(p * np.log2(p)) if not split.empty else 0\n",
    "        elif self.criterion == 'gini':\n",
    "            return 1 - np.sum(p ** 2)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"MyTreeClf class: max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, max_leafs={self.max_leafs}, bins={self.bins}\"\n",
    "    \n",
    "    def sum_leafs(self):\n",
    "        return self.__sum_tree_values\n",
    "    \n",
    "    def replace_leafs(self, X, y, loss=\"MSE\"):\n",
    "        queue = []\n",
    "        queue.append(self.tree)\n",
    "        while (queue):\n",
    "            node = queue.pop(0)\n",
    "            if (node and node.value_leaf):\n",
    "                idx = node.idx\n",
    "                y_pred = self.predict(X.loc[idx, :])\n",
    "                loss_value = y[idx] - y_pred\n",
    "                node.value_leaf = loss_value.mean() if loss == 'MSE' else loss_value.median()\n",
    "            queue.append(node.left)\n",
    "            queue.append(node.right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBoostReg:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=5, min_samples_split=2, max_leafs=20, bins=16, loss='MSE', metric=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_leafs = max_leafs\n",
    "        self.bins = bins\n",
    "        self.loss = loss\n",
    "        self.metric = metric\n",
    "\n",
    "        self.pred_0 = None\n",
    "        self.trees = []\n",
    "        self.best_score = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MyBoostReg class: n_estimators={self.n_estimators}, learning_rate={self.learning_rate}, max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, max_leafs={self.max_leafs}, bins={self.bins}'\n",
    "\n",
    "    def calc_score(self, y, y_pred):\n",
    "        if self.metric == 'MSE':\n",
    "            return np.sum((y - y_pred)**2) / len(y)\n",
    "        elif self.metric == 'MAE':\n",
    "            return np.sum(np.abs((y - y_pred))) / len(y)\n",
    "        elif self.metric == 'RMSE':\n",
    "            return np.sqrt(np.sum((y - y_pred)**2) / len(y)) \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.pred_0 = y.mean() if self.loss == 'MSE' else y.median()\n",
    "        Fm = self.pred_0\n",
    "        for _ in range(self.n_estimators):\n",
    "            rm = 2*(Fm - y)\n",
    "            tree = MyTreeReg(self.max_depth, self.min_samples_split, self.max_leafs, self.bins)\n",
    "            tree.fit(X, rm)\n",
    "            tree.replace_leafs(X, y, self.loss)\n",
    "            self.trees.append(tree)\n",
    "            y_pred = self.learning_rate * tree.predict(X)\n",
    "            Fm += y_pred\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "        self.best_score = self.metric(y, y_pred)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = 0\n",
    "        for tree in self.trees:\n",
    "            y_pred += tree.predict(X)\n",
    "        y_pred = self.learning_rate * y_pred + self.pred_0\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.value_split = None\n",
    "        self.value_leaf = None\n",
    "        self.side = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.idx = None\n",
    "\n",
    "class MyTreeReg:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, max_leafs=20, bins=None, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_leafs = max_leafs\n",
    "        self.leafs_cnt = 1\n",
    "        self.bins = bins\n",
    "        self.__sum_tree_values = 0\n",
    "        self.split_values = {}\n",
    "        self.criterion = criterion\n",
    "        self.fi = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = None\n",
    "        self.fi = { col: 0 for col in X.columns }\n",
    "        \n",
    "        def create_tree(root, X_root, y_root, side='root', depth=0):\n",
    "            if root is None:\n",
    "                root = Node()\n",
    "            col_name, split_value, ig = self.get_best_split(X_root, y_root)\n",
    "\n",
    "            mean_value = y_root.mean()\n",
    "\n",
    "            if depth >= self.max_depth or \\\n",
    "              len(y_root) < self.min_samples_split or \\\n",
    "              (self.leafs_cnt > 1 and self.leafs_cnt >= self.max_leafs):\n",
    "                root.side = side\n",
    "                root.value_leaf = mean_value\n",
    "                root.idx = list(y_root.index)\n",
    "                self.__sum_tree_values += root.value_leaf\n",
    "                return root\n",
    "\n",
    "            self.fi[col_name] += len(y_root) / len(y) * ig\n",
    "\n",
    "            X_left = X_root.loc[X_root[col_name] <= split_value]\n",
    "            y_left = y_root.loc[X_root[col_name] <= split_value]\n",
    "\n",
    "            X_right = X_root.loc[X_root[col_name] > split_value]\n",
    "            y_right = y_root.loc[X_root[col_name] > split_value]\n",
    "\n",
    "            if len(X_left) == 0 or len(X_right) == 0:\n",
    "                root.side = side\n",
    "                root.value_leaf = mean_value\n",
    "                root.idx = list(y_root.index)\n",
    "                self.__sum_tree_values += root.value_leaf\n",
    "                return root\n",
    "\n",
    "            root.feature = col_name\n",
    "            root.value_split = split_value\n",
    "            self.leafs_cnt += 1\n",
    "\n",
    "            root.left = create_tree(root.left, X_left, y_left, 'left', depth + 1)\n",
    "            root.right = create_tree(root.right, X_right, y_right, 'right', depth + 1)\n",
    "\n",
    "            return root\n",
    "\n",
    "        self.tree = create_tree(self.tree, X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for _, row in X.iterrows():\n",
    "            node = self.tree\n",
    "            while node.feature is not None:\n",
    "                if row[node.feature] <= node.value_split:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            y_pred.append(node.value_leaf)\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def print_tree(self, node=None, depth=0):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        if node.feature is not None:\n",
    "            print(f\"{' ' * depth}{node.feature} > {node.value_split}\")\n",
    "            if node.left is not None:\n",
    "                self.print_tree(node.left, depth + 1)\n",
    "            if node.right is not None:\n",
    "                self.print_tree(node.right, depth + 1)\n",
    "        else:\n",
    "            print(f\"{' ' * depth}{node.side} = {node.value_leaf}\")\n",
    "\n",
    "    def get_best_split(self, X, y):\n",
    "        mse_0 = self.mse(y)\n",
    "\n",
    "        col_name = None\n",
    "        split_value = None\n",
    "        gain = -float('inf')\n",
    "\n",
    "        for col in X.columns:\n",
    "            if not (col in self.split_values.keys()):\n",
    "                x_unique_values = np.unique(X[col])\n",
    "                if self.bins is None or len(x_unique_values) - 1 < self.bins:\n",
    "                    self.split_values[col] = np.array([(x_unique_values[i - 1] + \\\n",
    "                    x_unique_values[i]) / 2 for i in range(1, len(x_unique_values))])\n",
    "                else:\n",
    "                    _, self.split_values[col] = np.histogram(X[col], bins=self.bins)\n",
    "\n",
    "            for split_value_i in self.split_values[col]:\n",
    "                mask = X[col] <= split_value_i\n",
    "                left_split, right_split = y[mask], y[~mask]\n",
    "\n",
    "                mse_left = self.mse(left_split)\n",
    "                mse_right = self.mse(right_split)\n",
    "\n",
    "                weight_left = len(left_split) / len(y)\n",
    "                weight_right = len(right_split) / len(y)\n",
    "\n",
    "                mse_i = weight_left * mse_left + weight_right * mse_right\n",
    "\n",
    "                gain_i = mse_0 - mse_i\n",
    "                if gain < gain_i:\n",
    "                    col_name = col\n",
    "                    split_value = split_value_i\n",
    "                    gain = gain_i\n",
    "\n",
    "        return col_name, split_value, gain\n",
    "            \n",
    "    def mse(self, t):\n",
    "        t_mean = np.mean(t)\n",
    "        return np.sum((t - t_mean) ** 2) / (len(t)+1e-15)\n",
    "    \n",
    "    def __node_rule(self, p, split=pd.Series()):\n",
    "        if self.criterion == 'entropy':\n",
    "            return -np.sum(p * np.log2(p)) if not split.empty else 0\n",
    "        elif self.criterion == 'gini':\n",
    "            return 1 - np.sum(p ** 2)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"MyTreeClf class: max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, max_leafs={self.max_leafs}, bins={self.bins}\"\n",
    "    \n",
    "    def sum_leafs(self):\n",
    "        return self.__sum_tree_values\n",
    "    \n",
    "    def replace_leafs(self, X, y, loss=\"MSE\"):\n",
    "        queue = []\n",
    "        queue.append(self.tree)\n",
    "        while (queue):\n",
    "            node = queue.pop(0)\n",
    "            if (node and node.value_leaf):\n",
    "                idx = node.idx\n",
    "                y_pred = self.predict(X.loc[idx, :])\n",
    "                loss_value = y[idx] - y_pred\n",
    "                node.value_leaf = loss_value.mean() if loss == 'MSE' else loss_value.median()\n",
    "            queue.append(node.left)\n",
    "            queue.append(node.right)\n",
    "\n",
    "    def get_leafs_idx(self):\n",
    "        stack = []\n",
    "        stack.append(self.tree)\n",
    "        idxs = []\n",
    "        while stack:\n",
    "            node = stack.pop()\n",
    "            if not node: continue\n",
    "            if (node.value_leaf):\n",
    "                idxs.append(node.idx)\n",
    "            stack.append(node.left)\n",
    "            stack.append(node.right)\n",
    "        return idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBoostClf:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=5, min_samples_split=2, max_leafs=20, bins=16):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_leafs = max_leafs\n",
    "        self.bins = bins\n",
    "\n",
    "        self.pred_0 = None\n",
    "        self.trees = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MyBoostClf class: n_estimators={self.n_estimators}, learning_rate={self.learning_rate}, max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, max_leafs={self.max_leafs}, bins={self.bins}\"\n",
    "\n",
    "    def __calc_gamma(self, y, p, leaf_idxs):\n",
    "        gamma = 0\n",
    "        for idx in leaf_idxs:\n",
    "            gamma += np.sum(y[idx] - p[idx]) / (np.sum(p[idx]*(1-p[idx])) + 1e-15)\n",
    "        return gamma\n",
    "\n",
    "\n",
    "    def fit(self, X, y, verbose=None):\n",
    "        self.pred_0 = y.mean()\n",
    "        p = np.array([self.pred_0 for _ in range(len(y))])\n",
    "        Fm = np.log(p/(1-p + 1e-15))\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = MyTreeReg(self.max_depth, self.min_samples_split, self.max_leafs, self.bins)\n",
    "            p = np.exp(np.log(Fm))/(1+np.exp(np.log(Fm)) + 1e-15)\n",
    "            r = y - p\n",
    "            tree.fit(X, r)\n",
    "            leaf_idxs = tree.get_leafs_idx()\n",
    "            gamma = self.__calc_gamma(y, p, leaf_idxs)\n",
    "            self.trees.append(tree)\n",
    "            gammam = tree.predict(X)\n",
    "            Fm += self.learning_rate * gammam\n",
    "\n",
    "            if (verbose and not i % verbose):\n",
    "                print(i+1, r)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKMeans:\n",
    "    def __init__(self, n_clusters=3, max_iter=10, n_init=3, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        self.centroids = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MyKMeans class: n_clusters={self.n_clusters}, max_iter={self.max_iter}, n_init={self.n_init}, random_state={self.random_state}\"\n",
    "    \n",
    "    def __distance(self, x, y):\n",
    "        return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "    def __generate_centroids(self, X):\n",
    "        np.random.seed(seed=self.random_state)\n",
    "        centroids = []\n",
    "        for _ in range(self.n_clusters):\n",
    "            centroid = np.array([np.random.uniform(low=X[feat].min(), high=X[feat].max()) for feat in X.columns])\n",
    "            centroids.append(centroid)\n",
    "        return np.array(centroids)\n",
    "    \n",
    "    def __distances(self, x, centroids):\n",
    "        return np.array([self.__distance(x, centroid) for centroid in centroids])\n",
    "\n",
    "    def __get_new_centroids(self, clusters, X):\n",
    "        clusters_dict = {cluster: [] for cluster in range(max(clusters)+1)}\n",
    "        for i in range(len(clusters)):\n",
    "            clusters_dict[clusters[i]].append(i)\n",
    "        \n",
    "        new_centroids = []\n",
    "        for cluster, rows in clusters_dict.items():\n",
    "            centroid = np.array([X.loc[rows, feat].mean() for feat in X.columns])\n",
    "            new_centroids.append(centroid)\n",
    "        return np.array(new_centroids)\n",
    "\n",
    "\n",
    "    def _fit(self, X):\n",
    "        centroids = self.__generate_centroids(X)\n",
    "        for _ in range(self.max_iter):\n",
    "            clusters = [self.__distances(x, centroids).argmin() for x in X.values]\n",
    "            new_centroids = self.__get_new_centroids(clusters, X)\n",
    "            \n",
    "            if (new_centroids == centroids).all():\n",
    "                break\n",
    "        if np.nan not in new_centroids:\n",
    "            self.centroids.append(new_centroids)\n",
    "    \n",
    "    def __wcss(self, centroids, X):\n",
    "        return np.sum([self.__distances(x, centroids).min()**2 for x in X.values])\n",
    "        \n",
    "    def __best_centroids(self, X):\n",
    "        wcsss = np.array([self.__wcss(centroids, X) for centroids in self.centroids])\n",
    "        print(wcsss)\n",
    "        return self.centroids[wcsss.argmin()], wcsss.min()\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        for _ in range(self.n_init):\n",
    "            self._fit(X)\n",
    "        self.cluster_centers_, self.inertia_ = self.__best_centroids(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clusters = [self.__distances(x, self.cluster_centers_).argmin() for x in X.values]\n",
    "        return np.array(clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=100, centers=5, n_features=5, cluster_std=2.5, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = [f'col_{col}' for col in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyKMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m k_means \u001b[38;5;241m=\u001b[39m \u001b[43mMyKMeans\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_clusters\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_init\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m})\n\u001b[0;32m      2\u001b[0m k_means\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#k_means.cluster_centers_, k_means.inertia_\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyKMeans' is not defined"
     ]
    }
   ],
   "source": [
    "k_means = MyKMeans(**{\"n_clusters\": 10, \"max_iter\": 10, \"n_init\": 3})\n",
    "k_means.fit(X)\n",
    "#k_means.cluster_centers_, k_means.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgglomerative:\n",
    "    def __init__(self, n_clusters=3):\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MyAgglomerative class: n_clusters={self.n_clusters}\"\n",
    "    \n",
    "    def __distance(self, x, y):\n",
    "        return np.sqrt(np.sum((x - y)**2))\n",
    "    \n",
    "    def find_min_dist(self, distances):\n",
    "        min_dist, min_ind = distances[0][0], (0, 0)\n",
    "        for i in range(len(distances)-1):\n",
    "            for j in range(i+1, len(distances)):\n",
    "                if distances[i][j] < min_dist:\n",
    "                    min_dist = distances[i][j]\n",
    "                    min_ind = (i, j)\n",
    "        return min_ind\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        distances = [[self.__distance(X.loc[i, :], X.loc[j, :]) for j in range(i+1, len(X))] for i in range(len(X)-1)]\n",
    "        min_dist_ind = self.find_min_dist(distances)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (99,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m my_agg \u001b[38;5;241m=\u001b[39m MyAgglomerative()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmy_agg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m, in \u001b[0;36mMyAgglomerative.fit_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     12\u001b[0m     distances \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__distance(X\u001b[38;5;241m.\u001b[39mloc[i, :], X\u001b[38;5;241m.\u001b[39mloc[j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X))] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) ]\n\u001b[1;32m---> 13\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (99,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "my_agg = MyAgglomerative()\n",
    "my_agg.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDBSCAN:\n",
    "    def __init__(self, eps=3, min_samples=3):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MyDBSCAN class: eps={self.eps}, min_samples={self.min_samples}\"\n",
    "    \n",
    "    def __distance(self, x, y):\n",
    "        return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "    def find_neighbours(self, x, X):\n",
    "        neighbours = []\n",
    "        for i, y in enumerate(X.values):\n",
    "            if self.__distance(x, y) <= self.eps:\n",
    "                neighbours.append(i)\n",
    "        return neighbours\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        num_clusters = 1\n",
    "        X['cluster'] = -1\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            sample = X.loc[i, :]\n",
    "            if sample['cluster'] != -1:\n",
    "                continue\n",
    "            neighbours = self.find_neighbours(sample, X)\n",
    "            if len(neighbours)-1 < self.min_samples:\n",
    "                X.loc[i, 'cluster'] = 0\n",
    "            else:\n",
    "                X.loc[i, 'cluster'] = num_clusters\n",
    "                while neighbours:\n",
    "                    neighbour = neighbours[0]\n",
    "                    neighbours.pop(0)\n",
    "                    sample = X.loc[neighbour, :]\n",
    "                    neighbours_neighbours = self.find_neighbours(sample, X)\n",
    "                    if sample['cluster'] == 0 or len(neighbours_neighbours)-1 < self.min_samples:\n",
    "                        X.loc[neighbour, 'cluster'] = num_clusters\n",
    "                    else:\n",
    "                        X.loc[neighbour, 'cluster'] = num_clusters\n",
    "                        neighbours.extend(neighbours_neighbours)\n",
    "            num_clusters += 1\n",
    "        return X['cluster'].T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 37], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dbscan = MyDBSCAN()\n",
    "my_dbscan.fit_predict(X).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPCA:\n",
    "    def __init__(self, n_components=3):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MyPCA class: n_components={self.n_components}\"\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X_meaned = X - X.mean()\n",
    "        cov_mat = X_meaned.cov()\n",
    "        W_pca = np.linalg.eigh(cov_mat)[1][:, -self.n_components:]\n",
    "        X_reduced = X_meaned @ W_pca\n",
    "        return X_reduced\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6078959 ,  0.51071017, -0.16382569],\n",
       "       [-0.17587616,  0.10941046,  0.8434211 ],\n",
       "       [-0.1271461 , -0.71560981,  0.26282746],\n",
       "       [-0.76344463, -0.3022094 , -0.09486829],\n",
       "       [-0.0227224 , -0.35181893, -0.42863006]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigh(X.cov())[1][:, -3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.022279</td>\n",
       "      <td>-1.305308</td>\n",
       "      <td>8.897126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.892941</td>\n",
       "      <td>-10.837080</td>\n",
       "      <td>-9.350529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.081455</td>\n",
       "      <td>4.182531</td>\n",
       "      <td>9.924896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.353359</td>\n",
       "      <td>0.510684</td>\n",
       "      <td>9.129610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.116619</td>\n",
       "      <td>-0.639272</td>\n",
       "      <td>10.768165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.548752</td>\n",
       "      <td>-3.562789</td>\n",
       "      <td>9.934569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-4.247022</td>\n",
       "      <td>-11.394326</td>\n",
       "      <td>-9.805331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-3.571845</td>\n",
       "      <td>-0.859642</td>\n",
       "      <td>7.572720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6.662169</td>\n",
       "      <td>-1.893058</td>\n",
       "      <td>-4.150153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.062648</td>\n",
       "      <td>0.944221</td>\n",
       "      <td>1.172708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2\n",
       "0  -0.022279  -1.305308   8.897126\n",
       "1  -0.892941 -10.837080  -9.350529\n",
       "2   7.081455   4.182531   9.924896\n",
       "3   4.353359   0.510684   9.129610\n",
       "4   2.116619  -0.639272  10.768165\n",
       "..       ...        ...        ...\n",
       "95  0.548752  -3.562789   9.934569\n",
       "96 -4.247022 -11.394326  -9.805331\n",
       "97 -3.571845  -0.859642   7.572720\n",
       "98  6.662169  -1.893058  -4.150153\n",
       "99  1.062648   0.944221   1.172708\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pca = MyPCA()\n",
    "my_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = y.unique()\n",
    "        n_classes = len(self._classes)\n",
    "        self._means = np.zeros((n_classes, n_features))\n",
    "        self._vars = np.zeros((n_classes, n_features))\n",
    "        self._priors = np.zeros((n_classes, n_features))\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y==c]\n",
    "            self._means[idx] = X_c.mean(axis=0)\n",
    "            self._vars[idx] = X_c.var(axis=0)\n",
    "            self._priors[idx] = len(X_c) / len(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            posterior = np.log(self._priors[idx])\n",
    "            posterior += np.sum(np.log(self._pdf(idx, x)))\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, idx, x):\n",
    "        return 1/np.sqrt(2*np.pi*self._vars[idx]**2)*np.exp(-(x - self._means[idx])**2 / 2*(self._vars[idx]**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, n_iters=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.activation_fuction = lambda x: np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features, n_samples = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y_ = np.where(y > 0, 1, 0)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x in enumerate(X):\n",
    "                y_pred = self.activation_fuction(self.weights @ x + self.bias)\n",
    "                update = self.learning_rate * (y_[idx] - y_pred)\n",
    "                self.weights += update * x\n",
    "                self.bias += update\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.activation_fuction(self.weights @ X + self.bias)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    acc = np.sum(y == y_pred) / len(y)\n",
    "    return acc\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=150, n_features=2, centers=2, cluster_std=1.05, random_state=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x in enumerate(X):\n",
    "                condition = y_[idx] * (x @ self.w - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.learning_rate * 2 * self.lambda_param * self.w\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - x @ y_[idx])\n",
    "                    self.b -= self.learning_rate * y_[idx]\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = X @ self.w - self.b\n",
    "        return np.sign(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
